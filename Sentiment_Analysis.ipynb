{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nidhi76/Sentiment-Analysis/blob/master/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ir_eRnOIbTTz"
      },
      "source": [
        "## Sentiment analysis\n",
        "It is a natural language processing problem where text is understood and the underlying intent is predicted. Here,the sentiment of movie reviews as either positive or negative in Python  is predicted using the Keras deep learning library.\n",
        "\n",
        "## Data description\n",
        "The dataset is the Large Movie Review Dataset often referred to as the IMDB dataset.\n",
        "\n",
        "The [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/) (often referred to as the IMDB dataset) contains 25,000 highly polar movie reviews (good or bad) for training and the same amount again for testing. The problem is to determine whether a given movie review has a positive or negative sentiment.  Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zGs6auYKbvZ9"
      },
      "source": [
        "## Loading dataset\n",
        "First, we will load complete dataset and analyze some properties of it.<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QqXjhXycZqpP",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot\n",
        "import numpy\n",
        "import keras\n",
        "from keras import regularizers,layers\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7lUt9_CxbwGH",
        "colab": {}
      },
      "source": [
        "# np.load is used inside imdb.load_data. But imdb.load_data still assumes the default \n",
        "# values of an older version of numpy. So necessary changes to np.load are made\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "\n",
        "# load Numpy\n",
        "np_load_old = np.load\n",
        "\n",
        "# modify the default parameters of np.load\n",
        "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ln3lVLwypnp7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "c9b75735-4343-4fd0-88fb-154b03c6b50c"
      },
      "source": [
        "# call load_data with allow_pickle implicitly set to true\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)\n",
        "\n",
        "# restore np.load for future normal usage\n",
        "np.load = np_load_old\n",
        "\n",
        "X = np.concatenate((X_train, X_test), axis=0)\n",
        "y = np.concatenate((y_train, y_test), axis=0)\n",
        "\n",
        "print(X.shape)\n",
        "print(X_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000,)\n",
            "(25000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CDAIVK_rcQCr"
      },
      "source": [
        "## **Let's see some of reviews.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QR3E0YIQcQtg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "0dae4b74-4eef-453e-a23c-8bcc798d8c04"
      },
      "source": [
        "word_to_id = keras.datasets.imdb.get_word_index()\n",
        "id_to_word = {value:key for key,value in word_to_id.items()}\n",
        "for i in range(15,20):\n",
        "  print(\"********************************************\")\n",
        "  print(' '.join(id_to_word.get(id - 3, '?')for id in X_train[i] ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n",
            "********************************************\n",
            "? a total waste of time just throw in a few explosions non stop fighting exotic cars a deranged millionaire slow motion computer generated car crashes and last but not least a hugh ? like character with wall to wall hot babes and mix in a ? and you will have this sorry excuse for a movie i really got a laugh out of the dr evil like heavily ? compound the plot was somewhere between preposterous and non existent how many ? are willing to make a 25 million dollar bet on a car race answer 4 but didn't they become ? through ? responsibility this was written for ? males it plays like a video game i did enjoy the ? ii landing in the desert though\n",
            "********************************************\n",
            "? laputa castle in the sky is the bomb the message is as strong as his newer works and more pure fantastic and flying pirates how could it be any better the art is totally amazing and the soundtrack which is ? many times after this im not sure if this was the first time i heard it and evokes in me the most emotional sentimental response of any movie soundtrack ? the female lead in this movie is totally awesome and the boy ? is also a great role model he lives on his own the plot is classic miyazaki i won't give it away but the end is really great i rank this as one of miyazaki's three best with ? and spirited away also you may want to check out ? moving castle when it comes out sometime next year i hope if you like miyazaki check this one out as it readily available in the usa enjoy ? a\n",
            "********************************************\n",
            "? at the height of the ? big ? racism row in 2007 involving ? ? and the late ? ? i condemned on an internet forum those ? b b ' fans who praised the show after years of bashing ? ? sitcoms such as ? ? ? ? ? i thought they were being ? and said so ? ain't half hot ? was then thrown into the argument with some pointing out it had starred an english actor ? up well yes but michael bates had lived in india as a boy and spoke ? ? the show's ? overlook the reality he brought to his performance as ? ? ? the noted indian character actor ? ? said in a 1995 documentary ? ? the ? that he was upset when he heard bates had landed the role but added no indian actor could have played that role as well as bates indeed br br ? was perry and ? companion show to ? ? also set in wartime the ? english town of ? on sea had been replaced by the hot steamy ? of india in particularly a place called ? where an army concert party puts on shows for the troops among them ? ? george ? his first sitcom role since ? in ? camp ? ? ? melvyn hayes ? ? ? ? ? de ? ? graham john ? and ? ? the late christopher mitchell ? over this gang of ? was the ? sergeant major williams the brilliant ? davies who regarded them all as ? his frustration at not being able to lead his men up the jungle to engage the enemy in combat made him bitter and bullying though he was nice to ? whom he thought was his ? son then there was ever so english colonel reynolds donald ? and ? captain ? michael ? ? was like a wise old ? beginning each show by talking to the camera and closing them by ? obscure ? ? he loved being ? so much he came to regard himself as practically british his friends were the tea making ? ? the late ? ? who went on to ? your ? and the rope pulling ? ? ? ? so real indians featured in the show another point its ? ignore ? also provided what was described on the credits as ? ? similar to the ? songs used as incidental music on ? ? each edition closed with him ? ? of hope ? only to be ? by a ? up ' from williams the excellent opening theme was penned by jimmy perry and derek ? br br though never quite ? ? ? in the ? affections ? nevertheless was popular enough to run for a total of eight seasons in 1975 davies and ? topped the ? with a cover version of that old ? ? ? they then recorded an entire album of old ? entitled what else ? ? ' br br the show hit crisis point three years later when bates died of cancer rather than ? the role of ? the writers just let him be quietly forgotten when george ? left the character of ? took his place as ? providing another source of comedy br br the last edition in 1981 saw the soldiers leave india by boat for ? the ? ? watching them go with great sadness as did viewers br br repeats have been few and far between mainly on u k gold all because of its so called ? reputation this is strange for one thing the show was not specifically about racism if a white man ? up is so wrong why does david ? 1984 film 'a passage to ? still get shown on television it featured alec guinness as an indian and won two oscars it was derived from jimmy ? own experiences some characters were based on real people the sergeant major really did refer to his men as ? i take the view that if you are going to put history on television get it right ? the past no matter how ? it might seem to modern audiences is ? ? ? was both funny and truthful and viewers saw this thank heavens for d v d 's i say time to stop this review as williams would say i'll have no ? in this jungle\n",
            "********************************************\n",
            "? i have only had the luxury of seeing this movie once when i was rather young so much of the movie is ? in trying to remember it however i can say it was not as funny as a movie called killer tomatoes should have been and the most memorable things from this movie are the song and the scene with the elderly couple talking about poor timmy other than that the movie is really just scenes of little tomatoes and big tomatoes rolling around and people acting scared and overacting as people should do in a movie of this type however just having a very silly premise and a catchy theme song do not a good comedy make granted this movie is supposed to be a b movie nothing to be taken seriously however you should still make jokes that are funny and not try to ? a mildly amusing premise into a full ? movie perhaps a short would have been fine as the trailer showing the elderly couple mentioned above and a man desperately trying to gun down a larger ? was actually pretty good the trailer itself looked like a mock trailer but no they indeed made a full movie and a rather weak one at that\n",
            "********************************************\n",
            "? chances are is a charming romantic fantasy about a woman ? shepherd whose husband christopher ? is killed shortly after learning she is pregnant we then see the husband in heaven letting the powers that be know that he was taken too soon and that his wife needs him he is told he can return to earth but not as himself ? 19 years where we see ? daughter mary stuart masterson preparing to graduate from college and ? a young man robert downey jr who it turns out is the reincarnation of her father the film is a little on the predictable side the story goes all the places you expect it to but it is so ? played by an energetic cast especially shepherd and downey that you can't help but get wrapped up in the fun shepherd has rarely been seen on screen to better advantage and she and downey are backed by a talented group of character actors in supporting roles a lovely and charming fantasy that will ? and ? you\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oJRXGZDgdUAL"
      },
      "source": [
        "## Summarize the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "deletable": false,
        "id": "_ArrM01ldvAL",
        "nbgrader": {
          "checksum": "8b319945d93218bc4097b381a4866fc1",
          "grade": false,
          "grade_id": "cell-41893f0c3efb31e1",
          "locked": false,
          "schema_version": 1,
          "solution": true
        },
        "colab": {}
      },
      "source": [
        "def summarize_data():\n",
        "  \"\"\"\n",
        "  Output:\n",
        "                    classes: list, list of unique classes in y  \n",
        "                no_of_words: int, number of unique words in dataset x \n",
        "     list_of_review_lengths: list,  list of lengths of each review \n",
        "         mean_review_length: float, mean(list_of_review_lengths), a single floating point value\n",
        "          std_review_length: float, standard_deviation(list_of_review_lengths), a single floating point value\n",
        "  \"\"\"\n",
        "\n",
        "  list_of_review_lengths = []\n",
        "  n = []\n",
        "  Y = np.array(y_train)\n",
        "  classes = list(np.unique(Y))\n",
        "  for j in X:\n",
        "    for k in j:\n",
        "      n.append(k)\n",
        "  no_of_words = len(list(set(n)))\n",
        "  for i in X:\n",
        "    list_of_review_lengths.append(len(i))\n",
        "  a = np.array( list_of_review_lengths)  \n",
        "  mean_review_length = np.mean(list_of_review_lengths)\n",
        "  std_review_length = np.std( list_of_review_lengths)\n",
        "  return classes, no_of_words, list_of_review_lengths, mean_review_length, std_review_length\n",
        "\n",
        "\n",
        "classes, no_of_words, list_of_review_lengths, mean_review_length, std_review_length = summarize_data()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wNyXu7gzgXVf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "beaa6066-0863-46f0-bb68-b9c533b3087a"
      },
      "source": [
        "y_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, ..., 0, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wT-EnTzt1ZZ1"
      },
      "source": [
        "## One hot encode the output data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "deletable": false,
        "id": "ag1bLU021mwQ",
        "nbgrader": {
          "checksum": "387e9a11fdb2f5f630aa358e57c3e007",
          "grade": false,
          "grade_id": "cell-25ffdbcc436121e4",
          "locked": false,
          "schema_version": 1,
          "solution": true
        },
        "colab": {}
      },
      "source": [
        "def one_hot(y):\n",
        "  \"\"\"\n",
        "  Inputs:\n",
        "    y: numpy array with class labels\n",
        "  Outputs:\n",
        "    y_oh: numpy array with corresponding one-hot encodings\n",
        "  \"\"\"\n",
        "  y_oh = np.zeros((y.shape[0], 2)) \n",
        "  for i in range(y.shape[0]):\n",
        "    if y[i]==0:\n",
        "      y_oh[i][0]=1\n",
        "    else:\n",
        "      y_oh[i][1]=1\n",
        "  return y_oh\n",
        "y_train = one_hot(y_train)\n",
        "y_test = one_hot(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fjgVZKngfRLd"
      },
      "source": [
        "### Multi-hot encode the input data\n",
        "\n",
        "All sequences are of different length and our vocabulory size is 10K.  <br>\n",
        "\n",
        "1) Intialize vector of dimension 10,000 with value 0. <br>\n",
        "2) For those tokens in a sequence which are present in Vocabulary make that position as 1 and keep all other positions filled with 0. <br>\n",
        "For example, lets take Vocabulary = ['I': 0, ':1, 'eat: 2:' mango: 3, 'fruit':4, 'happy':5, 'you':6] <br>\n",
        "We have two sequnces and \n",
        "Multi-hot encoding of both sequences will be of dimension:  7 (vocab size).<br>\n",
        "1) *Mango is my favourite fruit* becomes *Mango ? ? ? fruit* after removing words which are not in my vocabulary. Hence multi hot encoding will have two 1's corresponding to mango and fruit i.e, [0, 0, 0, 1, 1, 0, 0] <br>\n",
        "Similarly, <br>\n",
        "  2) *I love to eat mango*  = *I ? ? eat mango*  =  [1, 1, 0, 1, 0, 0, 0]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "deletable": false,
        "id": "EGCeyTiUTn0F",
        "nbgrader": {
          "checksum": "6464e41d2c2e970a418b5c31dada227e",
          "grade": false,
          "grade_id": "cell-9621fa777a6091d7",
          "locked": false,
          "schema_version": 1,
          "solution": true
        },
        "colab": {}
      },
      "source": [
        "def multi_hot_encode(sequences, dimension):\n",
        "  \"\"\"\n",
        "    Input:\n",
        "          sequences: list of sequences in X_train or X_test\n",
        "\n",
        "    Output:\n",
        "          results: mult numpy matrix of shape(len(sequences), dimension)\n",
        "                  \n",
        "  \"\"\"\n",
        "  \n",
        "  \n",
        "  results = np.zeros((len(sequences), dimension))\n",
        "  for i in range(sequences.shape[0]):\n",
        "    for j in sequences[i]:\n",
        "      \n",
        "      results[i][j-1] = 1\n",
        "      \n",
        "  \n",
        "  return results\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "38PMImZBTn94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "b70c0f90-627b-4379-9e87-feb627bce1a0"
      },
      "source": [
        "x_train = multi_hot_encode(X_train, 10000)\n",
        "x_test = multi_hot_encode(X_test, 10000)\n",
        "\n",
        "print(\"x_train \", x_train.shape)\n",
        "print(\"x_test \", x_test.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train  (25000, 10000)\n",
            "x_test  (25000, 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mHMmE0Bcyyvr"
      },
      "source": [
        "## Splitting the data into train and validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bObjhuuUewPs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "1258de2b-caf9-40dc-bb81-3f4516e60bd9"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_strat, x_dev, y_strat, y_dev = train_test_split(x_train, y_train,test_size=0.40,random_state=0, stratify=y_train)\n",
        "x_strat.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15000, 10000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "W-oxp-ZchFin"
      },
      "source": [
        "## Building Model\n",
        "Building a multi layered feed forward network in keras. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SyIsmvSejTK5"
      },
      "source": [
        "### Creating the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "deletable": false,
        "id": "qVkxB48rijyA",
        "nbgrader": {
          "checksum": "896570e2d65bfdffc030e1b27e78c7e1",
          "grade": false,
          "grade_id": "cell-39c47f89f7634b15",
          "locked": false,
          "schema_version": 1,
          "solution": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "387c8d15-c5a6-4db7-ab59-ad0ebadffb23"
      },
      "source": [
        "def create_model():\n",
        "    \"\"\"\n",
        "    Output:\n",
        "        model: A compiled keras model\n",
        "    \"\"\"\n",
        "    import keras\n",
        "    from keras.models import Sequential\n",
        "    from keras.layers import Input, Activation\n",
        "    from keras import optimizers\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(15000, 32, input_length=10000))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(200, activation='relu'))\n",
        "    model.add(Dense(2, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "  \n",
        "model = create_model()\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 10000, 32)         480000    \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 320000)            0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 200)               64000200  \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 402       \n",
            "=================================================================\n",
            "Total params: 64,480,602\n",
            "Trainable params: 64,480,602\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "afBSknLSjXRB"
      },
      "source": [
        "### Fit the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "deletable": false,
        "id": "eJ8r5bfeiuON",
        "nbgrader": {
          "checksum": "e3871b03408d711869d928b6f49b774b",
          "grade": false,
          "grade_id": "cell-54b19898dc10302f",
          "locked": false,
          "schema_version": 1,
          "solution": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "outputId": "62f74c8d-1fb2-4560-bd6b-e659d8628eeb"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def fit(model):\n",
        "    \"\"\"\n",
        "    Action:\n",
        "        Fit the model created above using training data as x_strat and y_strat\n",
        "        and validation_data as x_dev and y_dev, verbose=2 and store it in 'history' variable.\n",
        "        \n",
        "        evaluate the model using x_test, y_test, verbose=0 and store it in 'scores' list\n",
        "    Output:\n",
        "        scores: list of length 2\n",
        "        history_dict: output of history.history where history is output of model.fit()\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    history = model.fit(x_strat, y_strat, validation_data = (x_dev, y_dev), verbose =2, epochs = 5) \n",
        "    scores = model.evaluate(x_test, y_test, verbose =0 )\n",
        "    history_dict = history.history \n",
        "    return scores,history_dict\n",
        "    \n",
        "scores,history_dict = fit(model)    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "469/469 - 37s - loss: 0.5291 - accuracy: 0.7677 - val_loss: 0.2945 - val_accuracy: 0.8803\n",
            "Epoch 2/5\n",
            "469/469 - 36s - loss: 0.2073 - accuracy: 0.9200 - val_loss: 0.2934 - val_accuracy: 0.8786\n",
            "Epoch 3/5\n",
            "469/469 - 37s - loss: 0.1154 - accuracy: 0.9583 - val_loss: 0.3659 - val_accuracy: 0.8711\n",
            "Epoch 4/5\n",
            "469/469 - 37s - loss: 0.0568 - accuracy: 0.9813 - val_loss: 0.4785 - val_accuracy: 0.8727\n",
            "Epoch 5/5\n",
            "469/469 - 37s - loss: 0.0252 - accuracy: 0.9920 - val_loss: 0.6138 - val_accuracy: 0.8662\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pXsHW0SJsELv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "59abc0d9-e1e4-4226-a33d-26416b77b05f"
      },
      "source": [
        "Accuracy=scores[1]*100\n",
        "print('Accuracy of your model is')\n",
        "print(scores[1]*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of your model is\n",
            "86.39600276947021\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}